length(unlist(strsplit(top_genes$NCBI_ID[k],";"))
)
j=1
GS<-as.numeric(unlist(strsplit(top_genes$grantham_score[k] ,";"))[j])
if(GS>150){
GS_call<-"Radical"
}ifelse(GS>101){
GS_call<-"Moderately radical"
}ifelse(GS>0){GS_call<-"Conservative"}else(GS_call<-"NA")
GS<-as.numeric(unlist(strsplit(top_genes$grantham_score[k] ,";"))[j])
if(GS>150){
GS_call<-"Radical"
}else if(GS>101){
GS_call<-"Moderately radical"
}else if(GS>0){GS_call<-"Conservative"}else(GS_call<-"NA")
GS_call
GS_text=paste(GS_Call," (",GS,")",sep="")
GS_text=paste(GS_call," (",GS,")",sep="")
GS_text
NCBI<-unlist(strsplit(top_genes$NCBI_ID[k] ,";"))[j]
if(NCBI!="."){NCBI=paste("\\href{https://www.ncbi.nlm.nih.gov/nuccore/", NCBI, "}{", NCBI, "}", sep = "")
}
cat(paste("{\\bf NCBI Change:} & ", NCBI ," & ", unlist(strsplit(top_genes$NT_change[k] ,";"))[j]  , " & {\\bf AA Change:} & ", unlist(strsplit(top_genes$AA_change[k] ,";"))[j] , "& {\\bf GS:}", GS_text," \\\\ ", sep = ""))
for(j in 1:length(unlist(strsplit(top_genes$NCBI_ID[k],";")))){
GS<-as.numeric(unlist(strsplit(top_genes$grantham_score[k] ,";"))[j])
if(GS>150){
GS_call<-"Radical"
}else if(GS>101){
GS_call<-"Moderately radical"
}else if(GS>0){GS_call<-"Conservative"}else(GS_call<-"NA")
GS_text=paste(GS_call," (",GS,")",sep="")
NCBI<-unlist(strsplit(top_genes$NCBI_ID[k] ,";"))[j]
if(NCBI!="."){NCBI=paste("\\href{https://www.ncbi.nlm.nih.gov/nuccore/", NCBI, "}{", NCBI, "}", sep = "")
}
cat(paste("{\\bf NCBI Change:} & ", NCBI ," & ", unlist(strsplit(top_genes$NT_change[k] ,";"))[j]  , " & {\\bf AA Change:} & ", unlist(strsplit(top_genes$AA_change[k] ,";"))[j] , "& {\\bf GS:}", GS_text," \\\\ ", sep = ""))
}
if(top_genes$Interpro_domain_clean[k]!="."){
cat(paste("{\\bf Interpro:} & \\multicolumn{5}{m{13cm}}{", top_genes$Interpro_domain_clean[k],  "} \\\\", sep = ""))
}
top_genes$Interpro_domain_clean[k]
if(top_genes$description[k]!="."){
cat(paste("{\\bf Description:} & \\multicolumn{5}{m{13cm}}{", top_genes$description[k],  "} \\\\", sep = ""))
}
NCBI=paste("\\href{https://www.ncbi.nlm.nih.gov/nuccore/", NCBI, "}{", NCBI, "}", sep = "")
NCBI
NCBI_ID
cat(paste("{\\bf Description:} & \\multicolumn{5}{m{13cm}}{", top_genes$description[k],  "} \\\\", sep = ""))
which(targets.out$avsnp150=="rs1800734")
k=3
cov<-coverage[which(coverage$Gene.name==top_genes[k,"Gene_name_1"] | coverage$Gene.name==top_genes[k,"Gene_name_2"]),id.sample]
if(length(cov)==0){
cov="."
}
cov
j=1
GS<-as.numeric(unlist(strsplit(top_genes$grantham_score[k] ,";"))[j])
GS
is.na(GS)
if(GS>150|!is.na(GS)){
}
if(!is.na(GS)|GS>150){
}
is.na(GS)
if(is.na(GS)){print(i)}
unlist(strsplit(top_genes$NCBI_ID[k] ,";"))
k
top_genes$NCBI_ID
k=72
j
unlist(strsplit(top_genes$NCBI_ID[k] ,";"))[j]
top_genes$Gene_name_1_2[k]
j
cat(paste("{\\bf NCBI Change:} & ", NCBI ," & ", unlist(strsplit(top_genes$NT_change[k] ,";"))[j]  , " & {\\bf AA Change:} & ", unlist(strsplit(top_genes$AA_change[k] ,";"))[j] , "& {\\bf GS:}", GS_text," \\\\ ", sep = ""))
NCBI
NCBI<-unlist(strsplit(top_genes$NCBI_ID[k] ,";"))[j]
cat(paste("{\\bf NCBI Change:} & ", NCBI ," & ", unlist(strsplit(top_genes$NT_change[k] ,";"))[j]  , " & {\\bf AA Change:} & ", unlist(strsplit(top_genes$AA_change[k] ,";"))[j] , "& {\\bf GS:}", GS_text," \\\\ ", sep = ""))
top_genes$NCBI_ID[k]
NCBI<-unlist(strsplit(top_genes$NCBI_ID[k] ,";"))[j]
cat(paste("{\\bf NCBI Change:} & ", NCBI ," & ", unlist(strsplit(top_genes$NT_change[k] ,";"))[j]  , " & {\\bf AA Change:} & ", unlist(strsplit(top_genes$AA_change[k] ,";"))[j] , "& {\\bf GS:}", GS_text," \\\\ ", sep = ""))
gsub("NM_","NM\/_",NCBI)
gsub("NM_","NM\/_",NCBI)
gsub("NM_","NM"\\_"",NCBI)
gsub(NCBI,"NM_","NM"\\_"")
NCBI
man(gsub)
?gsub
gsub("NM_","NM\\_"",NCBI)
)
gsub("NM_","NM\\_",NCBI)
gsub("NM_","NM\\_",NCBI)
library(stringr)
str_replace_all(NCBI,fixed("_"), "\\_"))
str_replace_all(NCBI,fixed("_"), "\\_")
GS
GS=121
GS=121if(GS>101 & GS=<150)
if(GS>101 & GS=<150)
if(GS>101 & GS=<150){print("si")}
if(GS>101 & GS<=150){print("si")}
apellidos<-c("ortiz",
"soriano",
"espinoza",
"camacho",
"valdez",
"palomares",
"rivera",
"santiago",
"sagaceta",
"mejia",
"alcala",
"corona",
"garcia",
"rodriguez",
"razo",
"azamar",
"hernandez",
"vergara",
"ocampo",
"medina",
"cardenas",
"ovando",
"otero",
"diaz",
"cabrera",
"mendoza",
"flores",
"lagunes",
"macdonald",
"ramos",
"castro",
"oropeza",
"delgadillo",
"velazquez",
"flores",
"huacuja",
"castro",
"gil",
"martinez",
"magana",
"cedro",
"tanda",
"larrosa",
"calderon",
"de la fuente",
"hernandez",
"beltran",
"anaya",
"rios",
"romero",
"jimenez",
"ortega",
"ponce",
"de leon",
"hernandez",
"patino",
"martinez",
"aguilar",
"baca",
"peynado",
"barcenas",
"lopez",
"carrillo",
"patino",
"de la cruz",
"montoya",
"rivera",
"paredez",
"mendiola",
"soto",
"mirzaeicheshmeh",
"cruz",
"hernandez",
"nambo",
"venegas")
apellidos
table(apellidos)
sum(table(apellidos)>1)
length(apellidos)
apellidos[table(apellidos)>1]
apellidos
apellidos==otero
apellidos=='otero'
apellidos=='otero'
sum(apellidos=='otero')
table(apellidos)>1
table(apellidos)
install.packages("fitdc")
library("fitdc")
read_fit("~/Downloads/16562473351.fit")
ls()
rm(list=ls())
```{r}
plot(cars)
install.packages("wordcloud")
library("wordcloud")
install.packages("tm")
install.packages("tm")
library("tm")
install.packages("SnowballC")
library("SnowballC")
text <- readLines("r1.txt")
setwd("~/Github/ciengro/")
text <- readLines("r1.txt")
text <- readLines("r1.txt")
text <- readLines("r1.txt")
text <- readLines("r1.txt")
text <- readLines("r1.txt")
text <- read.table("r1.txt")
text <- read.table("r1.txt")
text <- readline("r1.txt")
text <- readline("r1.txt")
```
text <- readPlain("r1.txt")
text <- readLines("r1.txt")
text
text <- readLines("r1.txt",)
text <- readLines("r1.txt")
text
rm(list=text)
rm(list=c(text))
text
rm(list=ls())
text <- readLines("r1.txt")
text
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 2)
findFreqTerms(dtm, lowfreq = 1)
findFreqTerms(dtm, lowfreq = 2)
findAssocs(dtm, terms = "ciencia", corlimit = 0.3)
```{r message=FALSE}
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 1)
d
findFreqTerms(dtm, lowfreq = 2)
text <- readLines("r2.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark1"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Greens"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Accent"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "BrBG"))
text <- readLines("r3.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Reds"))
text <- readLines("r4.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Reds"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1)
wordcloud(words = d$word, freq = d$freq, min.freq = 1)
install.packages("wordcloud2")
library("wordcloud2")
wordcloud2(d,figPath = "guerrero.png")
wordcloud2(d,figPath = "guerrero.png")
figPath=system.file("./guerrero.png",package = "wordcloud2"))
figPath=system.file("./guerrero.png",package = "wordcloud2")
wordcloud2(d,figPath = figPath)
figPath=system.file("./guerrero.png",package = "wordcloud2")
wordcloud2(d,figPath = figPath)
wordcloud2(d,figPath = figPath)
figPath="guerrero.png"
wordcloud2(d,figPath = figPath)
wordcloud2(d,figPath = figPath,color="yellow")
wordcloud2(d,figPath = figPath,color="yellow")
wordcloud2(d,figPath = figPath,color="yellow")
d
wordcloud2(d)
wordcloud2(d)
wordcloud2(d)
wordcloud2(d,shape = "star",hoverFunction = F)
wordcloud2(d,shape = "star")
wordcloud(d)
d
wordcloud(words = d$word, freq = d$freq, min.freq = 1)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,colors=brewer.pal(8,"Blues")
wordcloud(words = d$word, freq = d$freq, min.freq = 1,colors=brewer.pal(8,"Blues"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,colors=brewer.pal(3,"Blues"))
text <- readLines("r1.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
write.table(d,"r1.output.txt",quote=FALSE,row.names = FALSE)
text <- readLines("r2.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
write.table(d,"r2.output.txt",quote=FALSE,row.names = FALSE)
text <- readLines("r3.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
write.table(d,"r3.output.txt",quote=FALSE,row.names = FALSE)
text <- readLines("r4.txt")
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
write.table(d,"r4.output.txt",quote=FALSE,row.names = FALSE)
wordcloud2(demoFreq)
wordcloud2(demoFreq,figPath = "~/Downloads/twitter.png")
wordcloud2(demoFreq,figPath = "~/Downloads/twitter.png")
wordcloud2(demoFreq,figPath = "~/Downloads/twitter.png", size = 1.5,color = "skyblue")
wordcloud2(demoFreq,figPath = "~/Downloads/twitter.png", size = 1.5,color = "skyblue")
letterCloud(demoFreq,word="GUERRERO")
letterCloud(demoFreq,word="GUERRERO")
letterCloud(demoFreq,word="GUERRERO",size=2)
letterCloud(demoFreq,word="GUERRERO",size=2)
letterCloud(demoFreq,word="GUERRERO",wordSize = 1)
letterCloud(demoFreq,word="GUERRERO",wordSize = 1)
letterCloud(demoFreq, word = "WORDCLOUD2", wordSize = 1)
letterCloud(demoFreq, word = "WORDCLOUD2", wordSize = 1)
figPath = system.file("examples/t.png",package = "wordcloud2")
wordcloud2(demoFreq, figPath = figPath, size = 1.5,color = "skyblue")
wordcloud2(demoFreq,figPath = "~/Downloads/twitter.png", size = 1.5,color = "skyblue")
head(demoFreq)
library(limma)
library(minfi)
library(minfi)
library(IlluminaHumanMethylation450kanno.ilmn12.hg19)
library(IlluminaHumanMethylation450kmanifest)
ann450k = getAnnotation(IlluminaHumanMethylation450kanno.ilmn12.hg19)
targets <- read.metharray.sheet(dataDirectory, pattern=".idat")
dataDirectory="data/GSE67772_RAW/"
targets <- read.metharray.sheet(dataDirectory, pattern=".idat")
rm(list=ls())
library(limma)
library(minfi)
library(minfi)
library(IlluminaHumanMethylation450kanno.ilmn12.hg19)
library(IlluminaHumanMethylation450kmanifest)
dataDirectory="data/GSE67772_RAW/"
ann450k = getAnnotation(IlluminaHumanMethylation450kanno.ilmn12.hg19)
setwd("~/Github/Tools/Methylation/")
dataDirectory="data/GSE67772_RAW/"
targets <- read.metharray.sheet(dataDirectory)
dataDirectory="data/GSE67772_RAW/"
targets <- read.metharray.sheet(base = dataDirectory)
?read.450k.sheet
dataDirectory="data/GSE67772_RAW/"
targets <- read.metharray.sheet(dataDirectory)
dir("data/GSE67772_RAW/")
dataDirectory="data/GSE67772_RAW/"
targets <- read.metharray.sheet(dataDirectory)
